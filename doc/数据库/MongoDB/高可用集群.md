## MongoDB高可用集群

### 1. 复制集群

复制集群（Replica Sets），由一组拥有相同数据集的2个或两个以上的MongoDB实例组成的集群。

复制集群包括 Primary主节点、Secondary从节点和投票节点。

复制集群提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据可用性和安全性。

由一台Master服务，负责客户端写入操作，一台或多台服务做Slave，同步Master数据。一旦Master故障，可以快速在Slave中选出一台切换成Master。

整个集群中，只有主节点负责写操作，读操作不限制。

#### 1.1 优势

高可用：

- 防止设备故障
- 提供自动failover功能

灾难恢复：发生故障，可以从其他节点恢复数据，容灾备份

读写分离：

- 可以在从节点执行读操作，减轻主节点压力
- 从节点可以用于分析、报表、数据挖掘等后台任务

#### 1.2 原理

主节点需要记录所有变更数据库的操作，记录保存在 oplog 中，这个文件存储在 local 数据库，各个从节点同通过oplog来复制数据并应用于本地，保持与主节点数据一致。oplog具有幂等性，无论执行多少次结果都一致。

oplog日志结构

```json
{
    "ts":Timestamp(1446011233,2),
    "h":NumberLong("237267162816287"),
    "v":2,
    "op":"i",
    "ns":"hello.user",
    "o":{"_id":objectId("1232j2312b198d"),"name":"tom","age":"10"}
}
```

- ts：操作时间，当前时间戳+计数器，计数器每秒重置
- h：操作的全局唯一标识
- v：oplog版本信息
- op：操作类型
  - i：插入
  - u：更新
  - d：删除
  - c：执行命令
- n：空操作，特殊用途
- ns：操作针对的集合
- o：操作内容
- o2：更新查询条件，仅update操作包含该字段

复制集群数据同步分为：

- 初始化同步：全量从主节点同步数据。从节点一次加入会触发，从节点落后的数据量超过oplog的大小也会全量复制
- keep复制同步：初始化同步后，节点之间的同步是增量同步。

#### 1.3 心跳检测机制

主节点选举基于心跳触发。

整个集群需要保持一定的通信才能知道哪些节点活着哪些挂掉，主节点会向从节点每2秒发一次ping，如果其他节点在10秒内没有响应，就视为不能访问。每个节点内部维护一个状态映射表，包含节点角色、日志、时间戳等信息。如果主节点发现自己无法与大部分节点通信，则自己降为从节点。

主节点选举触发时机：

- 第一次初始化集群
- 从节点权重比主节点高时，发起替换选举
- 从节点发现集群中没有主节点时，发起选举
- 主节点不能访问大部分成员时主动降级

选举触发时，从节点尝试自己成为主节点。选举采用Raft协议，本质为二阶段过程+多数派协议。

##### 1.3.1 二阶段过程

第一阶段：

检测自身是否有被选举资格，如果符合资格就向其他节点发起本节点是否有选举资格的FreshnessCheck，进行同僚冲裁。

第二阶段：

发起者向集群中存活节点发送选举请求，仲裁者收到请求的节点会执行一系列合法性检查，如果检查通过，则仲裁者给发起者投一票。单个复制集群中最多50个节点，其中只有7个具有投票权

- 通过30秒选举锁防止一次选举中两次投票
- 使用terms（单调递增的计数器）来防止一次选举中投票两次

##### 1.3.2 多数派协议

发起者如果获得超过半数的投票，则选举通过，自身成为主节点。

相同优先级的节点通过第一阶段的同僚冲裁并进入第二阶段，会造成低于半数选票。因此，会sleep很短时间，再次尝试选举。

### 2. 复制集群搭建

#### 2.1 配置

```shell
#初始化集群数据文件目录和日志文件
mkdir -p /data/mongo/data/server1
mkdir -p /data/mongo/data/server2
mkdir -p /data/mongo/data/server3

mkdir -p /data/mongo/logs
touch /data/mongo/logs/server1.log
touch /data/mongo/logs/server2.log
touch /data/mongo/logs/server3.log
#创建集群配置文件目录
mkdir /root/mongocluster
```

主节点配置 mongo_37017.conf

```properties
tee /root/mongoCluster/mongo_37017.conf <<-'EOF'
#主节点配置
dbpath=/data/mongo/data/server1
bind_ip=0.0.0.0
port=37017
fork=true
logpath=/data/mongo/logs/server1.log
#集群名称
replSet=demoMongoCluster
EOF
```

从节点1配置 mongo_37018.conf

```properties
tee /root/mongoCluster/mongo_37018.conf <<-'EOF'
dbpath=/data/mongo/data/server2
bind_ip=0.0.0.0
port=37018
fork=true
logpath=/data/mongo/logs/server2.log
replSet=demoMongoCluster
EOF
```

从节点2配置 mongo_37019.conf

```properties
tee /root/mongoCluster/mongo_37019.conf <<-'EOF'
dbpath=/data/mongo/data/server3
bind_ip=0.0.0.0
port=37019
fork=true
logpath=/data/mongo/logs/server3.log
replSet=demoMongoCluster
EOF
```

启动集群脚本

```shell
tee /root/mongoCluster/start-mongo-cluster.sh <<-'EOF'
#! /bin/bash
/usr/local/mongo/bin/mongod -f /root/mongoCluster/mongo_37017.conf
/usr/local/mongo/bin/mongod -f /root/mongoCluster/mongo_37018.conf
/usr/local/mongo/bin/mongod -f /root/mongoCluster/mongo_37019.conf
echo "start mongo cluster..."
EOF

chmod 755 /root/mongoCluster/start-mongo-cluster.sh
```

关闭集群脚本

```shell
tee /root/mongoCluster/stop-mongo-cluster.sh <<-'EOF'
#! /bin/bash
/usr/local/mongo/bin/mongod --shutdown -f /root/mongoCluster/mongo_37017.conf
/usr/local/mongo/bin/mongod --shutdown -f /root/mongoCluster/mongo_37018.conf
/usr/local/mongo/bin/mongod --shutdown -f /root/mongoCluster/mongo_37019.conf
echo "stop mongo cluster..."
EOF

chmod 755 /root/mongoCluster/stop-mongo-cluster.sh
```

初始化集群，启动三个节点，进入主节点

```shell
mongo --host=127.0.0.1 --port=37017
```

执行命令

```sql
var cfg={"_id":"demoMongoCluster",
	"protocolversion":1,
	"members":[
        {"_id":1,"host":"127.0.0.1:37017","priority":10},
        {"_id":2,"host":"127.0.0.1:37018"},
    ]
}
rs.initiate(cfg)
rs.status()
```

节点动态增删

```sql
-- 增加
rs.add("127.0.0.1:37019")
-- 删除
rs.remove("127.0.0.1:37019")
```

### 3. 分片集群

分片（sharding）是将大型集群水平分割到不同服务器上。

#### 3.1 为什么分片

- 存储容量需求超出单机磁盘容量
- 活跃的数据集超出单机内存容量，导致很多请求都要从磁盘读取数据
- IOPS超出单个节点的服务能力，单机瓶颈越来越明显
- 副本集具有节点数量限制

#### 3.2 分片原理

分片集群由3个服务组成

- Shards Server：每个片由一个或多个mongo进程组成，用于存储数据
- Router Server：数据库集群的请求入口，所有请求都通过路由进行协调，不需要再应用程序添加路由，就是一个请求分发中心负责把程序的请求转发到对应shard服务器
- Config Server：配置服务器，存储所有数据库元信息配置

片键：shard key，为了在数据集合中分配文档，mongo使用分片主键分割集合。

区块：chunk，在一个Shards Server内部，mongo把数据分为区块，每个区块代表这个Shards Server内部一部分数据，包含基于分片主键的左闭右开的区间范围区块。

#### 3.3 分片策略

##### 3.3.1 范围分片

- 基于Shard Key的值切割数据，每一个Chunk将会分配到一个范围
- 范围分片适合满足在一定范围内的查找，查询效率高，
- 缺点是如果Shard Key有明显递增或递减趋势，则新插入的文档多会分布到同一个Chunk。

##### 3.3.2 hash分片

- hash分片是计算一个分片Shard Key的hash值，每个区块分配一个范围的hash值
- hash分片与范围分片互补，充分随机
- 缺点是范围查询性能不好，所有范围查询要分发到后端所有的Shard才能找出匹配的文档



